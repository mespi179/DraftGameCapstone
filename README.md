# DraftGameCapstone

## Objective
My goals for this project were simple, teach a computer to play a game through the use of reinforcement learning and see how model plays and performs against different strategies. My main goal for this project was to explore and understand reinforcement learning.

## The Game
The game we working to train, we have a set of 4 numbers available to choose from. There are two teams and each team takes turns selecting numbers on the board. When a number is selected by a team it is removed from the options and is no longer available for selection for the rest of the game. The scores of the game are calculated by taking the sum of the values a team has selected and the team with the highest total score wins the game. 
Here is an example of the game:  
Game: [62, 91, 63, 55]  
Player 1 selects 91  
Player 2 selects 63  
Player 1 selects 62  
Player 2 selects 55  
Player 1 score = 153, Player 2 score = 118  
Player 1 wins  


## Data Sources
For this particular project there is no data source. The algorithm generates its own data by repeatedly playing games against itself and collecting data from the outcomes of those games in order to learn. The only “data” provided outside of that generated by the algorithm is an arbitrary set of numbers, for example [63, 91, 63, 55]. The set of values I used can be found in value_sets.txt  
 
## Source Code and Parameter Settings
All the code required to run this project is in final-alphazerodraft.ipynb. A GPU is required in order to run this code. I used GPU services provided through kaggle to execute the notebook. Running the code to cell 13 will train the game on a single set of values ([62, 91, 63, 55]) and show the losses for that model. To train on a different set of values, you may change "vals = [62, 91, 63, 55]" in cell 12 to the values you would like to train. You can use cell 20 to train multiple models on the value_sets that I used. To save time and show examples, I have uploaded the models that I trained in the repository. Cell 22 can be used to read and evaluate the saved models. 
Parameters for this project were batch size, number of training iterations, number of monte carlo tree search simulations, number of full games/episodes to run during each iteration, number of iterations for train examples history, and epochs all of which can be found with their corresponding setting under the train() function in the notebook. Please feel free to play with the settings as this could possibly improve the performance of the model. 

## Results
The results.csv file shows a summary of the model's win rates across the value sets I used. Here, are the definitions of the column headings:  
Lg_lrw: Models win rate when playing against greedy strategy and model selects first  
Lg_grw: Greedy win rate when playing against greedy strategy and model selects first  
Lg_twr: Tie rate when playing against greedy strategy and model selects first  
Gl_lwr: Models win rate when playing against greedy strategy and greedy selects first  
Gl_gwr: Greedy win rate when playing against greedy strategy and greedy selects first  
Gl_twr: Tie rate when playing against greedy strategy and greedy selects first  
Lr_lwr: Models win rate when playing against random strategy and model selects first  
Lr_rwr: Random win rate when playing against random strategy and model selects first  
Lr_twr: Tie rate when playing against random strategy and model selects first  
Rl_lwr: Models win rate when playing against random strategy and random selects first  
Rl_rwr: Random win rate when playing against random strategy and random selects first  
Rl_twr: Tie rate when playing against random strategy and random selects first  
Vals: Value sets the model got trained on  
 
