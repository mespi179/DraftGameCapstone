{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import numpy as np\nimport math\nimport os\n\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport torch.optim as optim\n\nimport unittest\nfrom random import shuffle\nimport random\nimport pickle","metadata":{"execution":{"iopub.status.busy":"2022-07-30T03:40:30.025595Z","iopub.execute_input":"2022-07-30T03:40:30.026176Z","iopub.status.idle":"2022-07-30T03:40:30.030491Z","shell.execute_reply.started":"2022-07-30T03:40:30.026139Z","shell.execute_reply":"2022-07-30T03:40:30.029858Z"},"trusted":true},"execution_count":46,"outputs":[]},{"cell_type":"code","source":"class DraftGame:\n\n    def __init__(self, vals):\n        self.columns = 4\n        self.player_vals = vals\n        self.rounds = 0\n\n    def get_board(self):\n        b = np.zeros((self.columns,), dtype=np.int)\n        return b\n\n    def get_board_size(self):\n        return self.columns\n\n    def get_action_size(self):\n        return self.columns\n\n    \n    def next_state(self, board, player, action):\n        b = np.copy(board)\n        #print(\"round is: \", self.rounds,\"board is: \", b, \"player is: \", player, \"action is: \", action)\n        \n        b[action] = player\n      \n        #print(\"new board is: \", b, \"next player is: \", player)\n        \n        # Return new game state with perspective fliped\n        return (b, -player)\n\n    def legal_moves(self, board):\n        legal = False\n        for i in range(len(board)):\n            if board[i] == 0:\n                #print(\"checking for legal moves, board is: \", b, \"index is: \", i, \"value at index is: \", board[i] )\n                legal = True\n        return legal\n\n    def valid_moves(self, board):\n        # All moves are set to invalid\n        valid_moves = [0] * self.get_action_size()\n\n        for index in range(self.columns):\n            if board[index] == 0:\n                valid_moves[index] = 1\n\n        return valid_moves\n\n    def is_win(self, board, player):\n        playerscore = 0\n        otherscore = 0\n        if (self.legal_moves(board) == True):\n            return False\n        else:\n            for index in range(self.columns):\n                if board[index] == player:\n                    playerscore = playerscore + self.player_vals[index]\n                else:\n                    otherscore = otherscore + self.player_vals[index]\n\n            if playerscore > otherscore:\n                return True\n            else:\n                return False\n    def reward_for_player(self, board, player):\n        if self.is_win(board, player):\n            return 1\n        if self.is_win(board, -player):\n            return -1\n        if self.legal_moves(board):\n            return None\n\n        return 0\n\n    def canonical_board(self, board, player):\n        return player * board","metadata":{"execution":{"iopub.status.busy":"2022-07-30T03:40:30.154227Z","iopub.execute_input":"2022-07-30T03:40:30.154456Z","iopub.status.idle":"2022-07-30T03:40:30.167186Z","shell.execute_reply.started":"2022-07-30T03:40:30.154430Z","shell.execute_reply":"2022-07-30T03:40:30.166328Z"},"trusted":true},"execution_count":47,"outputs":[]},{"cell_type":"code","source":"class GameModel(nn.Module):\n\n    def __init__(self, board_size, action_size, device):\n\n        super(GameModel, self).__init__()\n\n        self.device = device\n        self.size = board_size\n        self.action_size = action_size\n\n        self.fc1 = nn.Linear(in_features=self.size, out_features=16)\n        self.fc2 = nn.Linear(in_features=16, out_features=16)\n\n        # Two heads: policy and value\n        self.action_head = nn.Linear(in_features=16, out_features=self.action_size)\n        self.value_head = nn.Linear(in_features=16, out_features=1)\n\n        self.to(device)\n\n    def forward(self, x):\n        x = F.relu(self.fc1(x))\n        x = F.relu(self.fc2(x))\n\n        action_logits = self.action_head(x)\n        value_logit = self.value_head(x)\n\n        return F.softmax(action_logits, dim=1), torch.tanh(value_logit)\n\n    def predict(self, board):\n        board = torch.FloatTensor(board.astype(np.float32)).to(self.device)\n        board = board.view(1, self.size)\n        self.eval()\n        with torch.no_grad():\n            pi, v = self.forward(board)\n\n        return pi.data.cpu().numpy()[0], v.data.cpu().numpy()[0]","metadata":{"execution":{"iopub.status.busy":"2022-07-30T03:40:30.269092Z","iopub.execute_input":"2022-07-30T03:40:30.269928Z","iopub.status.idle":"2022-07-30T03:40:30.280137Z","shell.execute_reply.started":"2022-07-30T03:40:30.269888Z","shell.execute_reply":"2022-07-30T03:40:30.279269Z"},"trusted":true},"execution_count":48,"outputs":[]},{"cell_type":"code","source":"def ucb_score(parent, child):\n    prior_score = child.prior * math.sqrt(parent.visit_count) / (child.visit_count + 1)\n    if child.visit_count > 0:\n        value_score = -child.value()\n    else:\n        value_score = 0\n\n    return value_score + prior_score\n\n\nclass Node:\n    def __init__(self, prior, to_play):\n        self.visit_count = 0\n        self.to_play = to_play\n        self.prior = prior\n        self.value_sum = 0\n        self.children = {}\n        self.state = None\n    \n    #True if node has been expanded\n    def expanded(self):\n        return len(self.children) > 0\n    \n    #gets value of node\n    def value(self):\n        if self.visit_count == 0:\n            return 0\n        return self.value_sum / self.visit_count\n    \n    def select_action(self, temperature):\n        \"\"\"\n        Select action according to the visit count distribution and the temperature.\n        \"\"\"\n        visit_counts = np.array([child.visit_count for child in self.children.values()])\n        actions = [action for action in self.children.keys()]\n        if temperature == 0:\n            action = actions[np.argmax(visit_counts)]\n        elif temperature == float(\"inf\"):\n            action = np.random.choice(actions)\n        else:\n            visit_count_distribution = visit_counts ** (1 / temperature)\n            visit_count_distribution = visit_count_distribution / sum(visit_count_distribution)\n            action = np.random.choice(actions, p=visit_count_distribution)\n\n        return action\n\n    def select_child(self):\n        #choose child with maximum UCB\n        best_score = -np.inf\n        best_action = -1\n        best_child = None\n\n        for action, child in self.children.items():\n            score = ucb_score(self, child)\n            if score > best_score:\n                best_score = score\n                best_action = action\n                best_child = child\n\n        return best_action, best_child\n    \n    def expand(self, state, to_play, action_probs):\n        \"\"\"\n        We expand a node and keep track of the prior policy probability given by neural network\n        \"\"\"\n        self.to_play = to_play\n        self.state = state\n        for a, prob in enumerate(action_probs):\n            if prob != 0:\n                self.children[a] = Node(prior=prob, to_play=self.to_play * -1)\n\n    def __repr__(self):\n        # to debug\n        prior = \"{0:.2f}\".format(self.prior)\n        return \"{} Prior: {} Count: {} Value: {}\".format(self.state.__str__(), prior, self.visit_count, self.value())\n\n\nclass MCTS:\n\n    def __init__(self, game, model, args):\n        self.game = game\n        self.model = model\n        self.args = args\n\n    def run(self, model, state, to_play):\n\n        root = Node(0, to_play)\n        # EXPAND root\n        #get priors from nueral network\n        action_probs, value = model.predict(state)\n        #get valid moves given state of the game\n        valid_moves = self.game.valid_moves(state)\n        action_probs = action_probs * valid_moves  # mask invalid moves\n        action_probs /= np.sum(action_probs)\n        root.expand(state, to_play, action_probs)\n\n        for _ in range(self.args['num_simulations']):\n            node = root\n            search_path = [node]\n\n            # SELECT\n            while node.expanded():\n                action, node = node.select_child()\n                search_path.append(node)\n\n            parent = search_path[-2]\n            state = parent.state\n            next_state, _ = self.game.next_state(state, player=1, action=action) \n            # Get the board from the perspective of the other player\n            next_state = self.game.canonical_board(next_state, player=-1)\n\n            # The value of the new state\n            value = self.game.reward_for_player(next_state, player=1)\n            if value is None:\n                # If the game has not ended, expand\n                action_probs, value = model.predict(next_state)\n                valid_moves = self.game.valid_moves(next_state)\n                action_probs = action_probs * valid_moves  # mask invalid moves\n                action_probs /= np.sum(action_probs)\n                node.expand(next_state, parent.to_play * -1, action_probs)\n\n            self.backpropagate(search_path, value, parent.to_play * -1)\n\n        return root\n\n    def backpropagate(self, search_path, value, to_play):\n        for node in reversed(search_path):\n            node.value_sum += value if node.to_play == to_play else -value\n            node.visit_count += 1","metadata":{"execution":{"iopub.status.busy":"2022-07-30T03:40:30.403732Z","iopub.execute_input":"2022-07-30T03:40:30.404290Z","iopub.status.idle":"2022-07-30T03:40:30.427435Z","shell.execute_reply.started":"2022-07-30T03:40:30.404256Z","shell.execute_reply":"2022-07-30T03:40:30.426724Z"},"trusted":true},"execution_count":49,"outputs":[]},{"cell_type":"code","source":"class Trainer:\n\n    def __init__(self, game, model, args):\n        self.game = game\n        self.model = model\n        self.args = args\n        self.mcts = MCTS(self.game, self.model, self.args)\n\n    def exceute_episode(self):\n\n        train_examples = []\n        current_player = 1\n        state = self.game.get_board()\n\n        while True:\n            canonical_board = self.game.canonical_board(state, current_player)\n\n            self.mcts = MCTS(self.game, self.model, self.args)\n            root = self.mcts.run(self.model, canonical_board, to_play=1)\n\n            action_probs = [0 for _ in range(self.game.get_action_size())]\n            for k, v in root.children.items():\n                action_probs[k] = v.visit_count\n\n            action_probs = action_probs / np.sum(action_probs)\n            train_examples.append((canonical_board, current_player, action_probs))\n\n            action = root.select_action(temperature=0)\n            state, current_player = self.game.next_state(state, current_player, action)\n            reward = self.game.reward_for_player(state, current_player)\n\n            if reward is not None:\n                ret = []\n                for hist_state, hist_current_player, hist_action_probs in train_examples:\n                    ret.append((hist_state, hist_action_probs, reward * ((-1) ** (hist_current_player != current_player))))\n\n                return ret\n\n    def learn(self):\n        pi_losses = []\n        v_losses = []\n        for i in range(1, self.args['numIters'] + 1):\n\n            print(\"{}/{}\".format(i, self.args['numIters']))\n\n            train_examples = []\n\n            for eps in range(self.args['numEps']):\n                iteration_train_examples = self.exceute_episode()\n                train_examples.extend(iteration_train_examples)\n\n            shuffle(train_examples)\n            pi_loss, v_loss = self.train(train_examples) #may need to append instead of assign index\n            pi_losses.append(pi_loss)\n            v_losses.append(v_loss)\n            filename = self.args['checkpoint_path']\n            self.save_checkpoint(folder=\".\", filename=filename)\n        return pi_losses, v_losses\n\n    def train(self, examples):\n        optimizer = optim.Adam(self.model.parameters(), lr=5e-4)\n        pi_losses = []\n        v_losses = []\n\n        for epoch in range(self.args['epochs']):\n            self.model.train()\n\n            batch_idx = 0\n\n            while batch_idx < int(len(examples) / self.args['batch_size']):\n                sample_ids = np.random.randint(len(examples), size=self.args['batch_size'])\n                boards, pis, vs = list(zip(*[examples[i] for i in sample_ids]))\n                boards = torch.FloatTensor(np.array(boards).astype(np.float64))\n                target_pis = torch.FloatTensor(np.array(pis))\n                target_vs = torch.FloatTensor(np.array(vs).astype(np.float64))\n\n                # predict\n                boards = boards.contiguous().cuda()\n                target_pis = target_pis.contiguous().cuda()\n                target_vs = target_vs.contiguous().cuda()\n\n                # compute output\n                out_pi, out_v = self.model(boards)\n                l_pi = self.loss_pi(target_pis, out_pi)\n                l_v = self.loss_v(target_vs, out_v)\n                total_loss = l_pi + l_v\n\n                pi_losses.append(float(l_pi))\n                v_losses.append(float(l_v))\n\n                optimizer.zero_grad()\n                total_loss.backward()\n                optimizer.step()\n\n                batch_idx += 1\n\n            print(\"Policy Loss\", np.mean(pi_losses))\n            print(\"Value Loss\", np.mean(v_losses))\n            print(\"Examples:\")\n            print(out_pi[0].detach())\n            print(target_pis[0])\n            return np.mean(pi_losses), np.mean(v_losses)\n\n    def loss_pi(self, targets, outputs):\n        loss = -(targets * torch.log(outputs)).sum(dim=1)\n        return loss.mean()\n\n    def loss_v(self, targets, outputs):\n        loss = torch.sum((targets-outputs.view(-1))**2)/targets.size()[0]\n        return loss\n\n    def save_checkpoint(self, folder, filename):\n        if not os.path.exists(folder):\n            os.mkdir(folder)\n\n        filepath = os.path.join(folder, filename)\n        torch.save({\n            'state_dict': self.model.state_dict(),\n        }, filepath)","metadata":{"execution":{"iopub.status.busy":"2022-07-30T03:40:30.567805Z","iopub.execute_input":"2022-07-30T03:40:30.568039Z","iopub.status.idle":"2022-07-30T03:40:30.592915Z","shell.execute_reply.started":"2022-07-30T03:40:30.568012Z","shell.execute_reply":"2022-07-30T03:40:30.592180Z"},"trusted":true},"execution_count":50,"outputs":[]},{"cell_type":"code","source":"#Train Model\n\ndevice = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n\n\nargs = {\n    'batch_size': 64,\n    'numIters': 500,                             # training iterations\n    'num_simulations': 100,                     # Number of MCTS simulations to run when deciding on a move to play\n    'numEps': 100,                              # Number of games to run for each iteration\n    'numItersForTrainExamplesHistory': 20,\n    'epochs': 2,                                # epochs of training / iteration\n    'checkpoint_path': 'latest.pth'             # Where to save most current set of weights\n}\n\nvals = [62, 91, 63, 55]\ngame = DraftGame(vals)\n\nboard_size = game.get_board_size()\n\naction_size = game.get_action_size()\n\nmodel = GameModel(board_size, action_size, device)\n\ntrainer = Trainer(game, model, args)\npi_losses, v_losses = trainer.learn()","metadata":{"execution":{"iopub.status.busy":"2022-07-30T03:40:30.743139Z","iopub.execute_input":"2022-07-30T03:40:30.744053Z","iopub.status.idle":"2022-07-30T03:41:10.376824Z","shell.execute_reply.started":"2022-07-30T03:40:30.744007Z","shell.execute_reply":"2022-07-30T03:41:10.375365Z"},"trusted":true},"execution_count":51,"outputs":[]},{"cell_type":"code","source":"#Plot Model Losses\nimport pandas as pd\nlosses = pd.DataFrame(pi_losses, v_losses).reset_index()\nlosses.columns = ['pi_loss', 'v_loss']\nlosses.reset_index()\nfigfile = str(str(vals)+'.png')\nlosses.plot.line(figsize=(16,8)).figure.savefig(figfile)\n\n","metadata":{"execution":{"iopub.status.busy":"2022-07-30T03:41:10.378406Z","iopub.execute_input":"2022-07-30T03:41:10.378817Z","iopub.status.idle":"2022-07-30T03:41:10.660241Z","shell.execute_reply.started":"2022-07-30T03:41:10.378773Z","shell.execute_reply":"2022-07-30T03:41:10.659542Z"},"trusted":true},"execution_count":52,"outputs":[]},{"cell_type":"markdown","source":"## Draft Game Test","metadata":{}},{"cell_type":"code","source":"#Draft Game Test\nb = np.zeros((4,), dtype=np.int)\nvals = [12, 10, 8, 4]\ngame = DraftGame(vals)\nplayer = 1\nlegal=True\nwhile legal == True:\n    canonical_b = game.canonical_board(b, player)\n    print(\"canonical: \", canonical_b, \"normal: \", b)\n    priors = model.predict(canonical_b)\n    priors = priors[0]\n    print(priors)\n    maxind = np.argmax(priors)\n    valid = game.valid_moves(b)\n    arg_sorted_priors = np.argsort(priors, axis=0)\n    if valid[arg_sorted_priors[-1]] == 1:\n        b, player = game.next_state(b, player, arg_sorted_priors[-1])\n    elif valid[arg_sorted_priors[-2]] ==1:\n        b, player = game.next_state(b, player, arg_sorted_priors[-2])\n    elif valid[arg_sorted_priors[-3]] ==1:\n        b, player = game.next_state(b, player, arg_sorted_priors[-3])\n    else:\n        b, player = game.next_state(b, player, arg_sorted_priors[-4])\n    legal = game.legal_moves(b)\n    #print('legal: ', legal)\n    #print(\"next player: \",player)\n    ","metadata":{"execution":{"iopub.status.busy":"2022-07-30T03:41:10.661396Z","iopub.execute_input":"2022-07-30T03:41:10.662939Z","iopub.status.idle":"2022-07-30T03:41:10.682118Z","shell.execute_reply.started":"2022-07-30T03:41:10.662894Z","shell.execute_reply":"2022-07-30T03:41:10.681365Z"},"trusted":true},"execution_count":53,"outputs":[]},{"cell_type":"markdown","source":"## Learned Strategy vs Random and Greedy","metadata":{}},{"cell_type":"code","source":"def learned_choice(model, game, b, player=1):\n    canonical_b = game.canonical_board(b, player)\n    priors = model.predict(canonical_b)\n    priors = priors[0]\n    maxind = np.argmax(priors)\n    valid = game.valid_moves(b)\n    arg_sorted_priors = np.argsort(priors, axis=0)\n    if valid[arg_sorted_priors[-1]] == 1:\n        b, player = game.next_state(b, player, arg_sorted_priors[-1])\n    elif valid[arg_sorted_priors[-2]] ==1:\n        b, player = game.next_state(b, player, arg_sorted_priors[-2])\n    elif valid[arg_sorted_priors[-3]] ==1:\n        b, player = game.next_state(b, player, arg_sorted_priors[-3])\n    else:\n        b, player = game.next_state(b, player, arg_sorted_priors[-4])\n    return b\n\ndef rand_choice(b, game, player = -1):\n    valid = game.valid_moves(b)\n    options = []\n    for i in range(len(valid)):\n        if valid[i] == 1:\n            options.append(i)\n    choice_index = random.choice(options)\n    new_board = np.copy(b)\n    new_board[choice_index] = -1\n    return new_board\n    \n\ndef greedy_choice(b, game, player = -1):\n    valid = game.valid_moves(b)\n    options = []\n    for i in range(len(valid)):\n        if valid[i] == 1:\n            options.append(i) #gives index of options [0, 2, 3]\n   \n    best_ind = options[0]\n    for i in options:\n        if game.player_vals[i] > game.player_vals[best_ind]:\n            best_ind = i\n    new_board = np.copy(b)\n    new_board[best_ind] = -1\n    return new_board","metadata":{"execution":{"iopub.status.busy":"2022-07-30T03:41:10.684277Z","iopub.execute_input":"2022-07-30T03:41:10.684640Z","iopub.status.idle":"2022-07-30T03:41:10.696430Z","shell.execute_reply.started":"2022-07-30T03:41:10.684603Z","shell.execute_reply":"2022-07-30T03:41:10.695557Z"},"trusted":true},"execution_count":54,"outputs":[]},{"cell_type":"code","source":"def against_rand(num_games, starting_player,model, game, vals):\n    learn_tally = 0\n    rand_tally = 0\n    tie_tally = 0\n    for i in range(num_games):\n        game = DraftGame(vals)\n        board = np.zeros((4,), dtype=np.int)\n        current_player = starting_player\n        keep_playing = True\n        while keep_playing == True:\n            if current_player == 1:\n                new = learned_choice(model, game, board, player=1)\n                next_player = -1\n            else:\n                new = rand_choice(board, game)\n                next_player = 1\n            current_player = next_player\n            board = np.copy(new)\n            if (0 not in board):\n                keep_playing = False\n                learn_winner = game.is_win(board, 1)\n                rand_winner = game.is_win(board, -1)\n                if learn_winner == True:\n                    #print(\"Learned Strategy Wins\")\n                    learn_tally = learn_tally+1\n                elif rand_winner == True:\n\n                    rand_tally = rand_tally + 1\n                else: \n                    tie_tally = tie_tally + 1\n    print(\"learned strategy win rate was: \", (learn_tally/num_games)*100, \"%\")\n    print(\"tie rate was: \", (tie_tally/num_games)*100, \"%\")\n    print(\"random strategy win rate was: \", (rand_tally/num_games)*100, \"%\")\n    return (learn_tally/num_games)\n    \ndef against_greedy(num_games, starting_player, model, game, vals):\n    learn_tally = 0\n    greedy_tally = 0\n    tie_tally = 0\n    for i in range(num_games):\n        #print(\"---------------------------------NEW GAME---------------------------------\")\n        #vals = [12, 10, 8, 4]\n        game = DraftGame(vals)\n        board = np.zeros((4,), dtype=np.int)\n        #print(board)\n        current_player = starting_player\n        keep_playing = True\n        while keep_playing == True:\n            #print(\"BEFORE: board: \", board, \"current player is: \", current_player)\n            if current_player == 1:\n                new = learned_choice(model, game, board)\n                next_player = -1\n            else:\n                new = greedy_choice(board, game)\n                next_player = 1\n            current_player = next_player\n            board = np.copy(new)\n            #print(board)\n            #print(\"After: new board: \", board, \"current player is now: \", current_player)\n            if (0 not in board):\n                keep_playing = False\n                learn_winner = game.is_win(board, 1)\n                greedy_winner = game.is_win(board, -1)\n                if learn_winner == True:\n                    #print(\"Learned Strategy Wins\")\n                    learn_tally = learn_tally+1\n                elif greedy_winner == True:\n                    #print(\"Other Strategy Wins\")\n                    greedy_tally = greedy_tally + 1\n                else: \n                    #print('Game Tied')\n                    tie_tally = tie_tally + 1\n    print(\"learned strategy win rate was: \", (learn_tally/num_games)*100, \"%\")\n    print(\"tie rate was: \", (tie_tally/num_games)*100, \"%\")\n    print(\"greedy strategy win rate was: \", (greedy_tally/num_games)*100, \"%\")\n    return (learn_tally/num_games)","metadata":{"execution":{"iopub.status.busy":"2022-07-30T03:41:10.697774Z","iopub.execute_input":"2022-07-30T03:41:10.698215Z","iopub.status.idle":"2022-07-30T03:41:10.716566Z","shell.execute_reply.started":"2022-07-30T03:41:10.698179Z","shell.execute_reply":"2022-07-30T03:41:10.715866Z"},"trusted":true},"execution_count":55,"outputs":[]},{"cell_type":"code","source":"def results(model, game, vals):\n    print('Against greedy -- learned going first: ')\n    against_greedy(5, 1, model, game, vals)\n    print('Against greedy -- greedy going first: ')\n    against_greedy(5, -1, model, game, vals)\n    print('Against rand -- learned going first: out of 100')\n    against_rand(100, 1, model, game, vals)\n    print('Against rand -- rand going first: out of 100')\n    against_rand(100, -1, model, game, vals)","metadata":{"execution":{"iopub.status.busy":"2022-07-30T03:41:10.717864Z","iopub.execute_input":"2022-07-30T03:41:10.718124Z","iopub.status.idle":"2022-07-30T03:41:10.730130Z","shell.execute_reply.started":"2022-07-30T03:41:10.718091Z","shell.execute_reply":"2022-07-30T03:41:10.729295Z"},"trusted":true},"execution_count":56,"outputs":[]},{"cell_type":"code","source":"def train(vals):\n    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n\n\n    args = {\n        'batch_size': 64,\n        'numIters': 500,                                # Total number of training iterations\n        'num_simulations': 100,                         # Total number of MCTS simulations to run when deciding on a move to play\n        'numEps': 100,                                  # Number of full games (episodes) to run during each iteration\n        'numItersForTrainExamplesHistory': 20,\n        'epochs': 2,                                    # Number of epochs of training per iteration\n        'checkpoint_path': 'latest.pth'                 # location to save latest set of weights\n    }\n\n    game = DraftGame(vals)\n\n    board_size = game.get_board_size()\n\n    action_size = game.get_action_size()\n\n    model = GameModel(board_size, action_size, device)\n\n    trainer = Trainer(game, model, args)\n    pi_losses, v_losses = trainer.learn()\n    \n    losses = pd.DataFrame(pi_losses, v_losses).reset_index()\n    losses.columns = ['pi_loss', 'v_loss']\n    losses.reset_index()\n    figfile = str(str(vals)+'.png')\n    losses.plot.line(figsize=(16,8)).figure.savefig(figfile)\n    \n    return model, game\n\n\ndef evaluate(model, vals):\n    game = DraftGame(vals)\n    print('FINAL RESULTS')\n    results(model, game, vals)","metadata":{"execution":{"iopub.status.busy":"2022-07-30T03:41:10.731333Z","iopub.execute_input":"2022-07-30T03:41:10.732175Z","iopub.status.idle":"2022-07-30T03:41:10.742217Z","shell.execute_reply.started":"2022-07-30T03:41:10.732140Z","shell.execute_reply":"2022-07-30T03:41:10.741393Z"},"trusted":true},"execution_count":57,"outputs":[]},{"cell_type":"code","source":"#Save Model\nmodel_name = str('model'+ str(vals))\nfilename = model_name\npickle.dump(model, open(model_name, 'wb'))","metadata":{"execution":{"iopub.status.busy":"2022-07-30T03:41:10.743685Z","iopub.execute_input":"2022-07-30T03:41:10.743952Z","iopub.status.idle":"2022-07-30T03:41:10.755462Z","shell.execute_reply.started":"2022-07-30T03:41:10.743918Z","shell.execute_reply":"2022-07-30T03:41:10.754788Z"},"trusted":true},"execution_count":58,"outputs":[]},{"cell_type":"code","source":"#To open value_sets.txt\nwith open(\"value_sets.txt\", \"rb\") as new_filename:\n    vals_list_test = pickle.load(new_filename)\nvals_list","metadata":{"execution":{"iopub.status.busy":"2022-07-30T03:41:10.758476Z","iopub.execute_input":"2022-07-30T03:41:10.758664Z","iopub.status.idle":"2022-07-30T03:41:10.768528Z","shell.execute_reply.started":"2022-07-30T03:41:10.758629Z","shell.execute_reply":"2022-07-30T03:41:10.767836Z"},"trusted":true},"execution_count":59,"outputs":[]},{"cell_type":"code","source":"#To train and save models for each value set in value_sets.txt\nfor vals in vals_list:\n    print('CURRENT VALS IS: ', vals)\n    model, game = train(vals)\n    #save models\n    strvals = str(vals)\n    model_name = str('model'+ str(vals))\n    filename = model_name\n    pickle.dump(model, open(model_name, 'wb'))\n    ","metadata":{"execution":{"iopub.status.busy":"2022-07-30T03:41:10.772425Z","iopub.execute_input":"2022-07-30T03:41:10.772708Z","iopub.status.idle":"2022-07-30T03:47:47.208006Z","shell.execute_reply.started":"2022-07-30T03:41:10.772683Z","shell.execute_reply":"2022-07-30T03:47:47.207170Z"},"trusted":true},"execution_count":60,"outputs":[]},{"cell_type":"code","source":"#To load and evaluate imported model\nwith open('model[62, 91, 63, 55]', 'rb') as file:  \n    model = pickle.load(file)\n\nevaluate(model, [62, 91, 63, 55])","metadata":{"execution":{"iopub.status.busy":"2022-07-30T03:47:47.209273Z","iopub.execute_input":"2022-07-30T03:47:47.209618Z","iopub.status.idle":"2022-07-30T03:47:47.407001Z","shell.execute_reply.started":"2022-07-30T03:47:47.209580Z","shell.execute_reply":"2022-07-30T03:47:47.406314Z"},"trusted":true},"execution_count":61,"outputs":[]}]}